# ðŸ’¬ InstructTTSEval
[![arXiv](https://img.shields.io/badge/arXiv-2506.16381-b31b1b.svg)](https://arxiv.org/abs/2506.16381)
[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Dataset-blue)](https://huggingface.co/datasets/CaasiHUANG/InstructTTSEval)

InstructTTSEval is a comprehensive benchmark designed to evaluate Text-to-Speech (TTS) systems' ability to follow complex natural-language style instructions. The dataset provides a hierarchical evaluation framework with three progressively challenging tasks that test both low-level acoustic control and high-level style generalization capabilities.

- Data available at [Huggingface](https://huggingface.co/datasets/CaasiHUANG/InstructTTSEval)
- Paper: [InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems](https://arxiv.org/pdf/2506.16381)
- Evaluation script: [`InstructTTSEval/eval`](https://github.com/KexinHUANG19/InstructTTSEval/tree/main/eval)

## Citation
Please cite our paper if you find this work useful:
```bibtex
@misc{huang2025instructttsevalbenchmarkingcomplexnaturallanguage,
      title={InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems}, 
      author={Kexin Huang and Qian Tu and Liwei Fan and Chenchen Yang and Dong Zhang and Shimin Li and Zhaoye Fei and Qinyuan Cheng and Xipeng Qiu},
      year={2025},
      eprint={2506.16381},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.16381}, 
}
```
